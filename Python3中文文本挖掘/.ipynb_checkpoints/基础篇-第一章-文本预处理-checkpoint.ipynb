{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter01 - 文本预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要应用jieba分词工具包进行分词\n",
    "\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    接下来我们以三体第一部为例，展示分词的一些常用技术方法，首先读取需要的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['《三体》\\n', '作者：刘慈欣\\n', '正文\\n', '前言\\n', '\\u3000\\u3000《三体》终于能与科幻朋友们见面了，用连载的方式事先谁都没有想到，也是无奈之举。之前就题材问题与编辑们仔细商讨过，感觉没有什么问题，但没想到今年是文革三十周年这事儿，单行本一时出不了，也只能这样了。\\n', '\\u3000\\u3000其实这本书不是文革题材的，文革内容在其中只占不到十分之一，但却是一个漂荡在故事中挥之不去的精神幽灵。\\n', '\\u3000\\u3000本书虽不是《球状闪电》的续集，但可以看做那个故事所发生的世界在其后的延续，那个物理学家在故事中出现但已不重要，其他的人则永远消失了，林云真的死了，虽然我有时在想，如果她活下来，最后是不是这个主人公的样子？\\n', '\\u3000\\u3000这是一个暂名为《地球往事》的系列的第一部，可以看做一个更长的故事的开始。\\n', '\\u3000\\u3000这是一个关于背叛的故事，也是一个生存与死亡的故事，有时候，比起生存还是死亡来，忠诚与背叛可能更是一个问题。\\n', '\\u3000\\u3000疯狂与偏执，最终将在人类文明的内部异化出怎样的力量？冷酷的星空将如何拷问心中道德？\\n', '\\u3000\\u3000作者试图讲述一部在光年尺度上重新演绎的中国现代史，讲述一个文明二百次毁灭与重生的传奇。\\n']\n"
     ]
    }
   ],
   "source": [
    "doc_path = \"./datasets/three_body_1.txt\"\n",
    "example_list = []\n",
    "\n",
    "with open(doc_path, mode='r') as f:\n",
    "    for i in range(20):\n",
    "        # print(f.readline())\n",
    "        line = f.readline()\n",
    "        if '\\n' != line:\n",
    "            example_list.append(line)\n",
    "\n",
    "print(example_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['《 三体 》 \\n', '作者 ： 刘慈欣 \\n', '正文 \\n', '前言 \\n', '\\u3000 \\u3000 《 三体 》 终于 能 与 科幻 朋友 们 见面 了 ， 用 连载 的 方式 事先 谁 都 没有 想到 ， 也 是 无奈 之 举 。 之前 就 题材 问题 与 编辑 们 仔细 商讨 过 ， 感觉 没有 什么 问题 ， 但 没想到 今年 是 文革 三十周年 这 事儿 ， 单行本 一时 出 不了 ， 也 只能 这样 了 。 \\n', '\\u3000 \\u3000 其实 这 本书 不是 文革 题材 的 ， 文革 内容 在 其中 只 占 不到 十分之一 ， 但 却是 一个 漂荡 在 故事 中 挥之不去 的 精神 幽灵 。 \\n', '\\u3000 \\u3000 本书 虽 不是 《 球状 闪电 》 的 续集 ， 但 可以 看做 那个 故事 所 发生 的 世界 在 其后 的 延续 ， 那个 物理学家 在 故事 中 出现 但 已 不 重要 ， 其他 的 人 则 永远 消失 了 ， 林云 真的 死 了 ， 虽然 我 有时 在 想 ， 如果 她 活 下来 ， 最后 是不是 这个 主人公 的 样子 ？ \\n', '\\u3000 \\u3000 这是 一个 暂 名为 《 地球 往事 》 的 系列 的 第一部 ， 可以 看做 一个 更长 的 故事 的 开始 。 \\n', '\\u3000 \\u3000 这是 一个 关于 背叛 的 故事 ， 也 是 一个 生存 与 死亡 的 故事 ， 有时候 ， 比起 生存 还是 死亡 来 ， 忠诚 与 背叛 可能 更是 一个 问题 。 \\n', '\\u3000 \\u3000 疯狂 与 偏执 ， 最终 将 在 人类文明 的 内部 异化 出 怎样 的 力量 ？ 冷酷 的 星空 将 如何 拷问 心中 道德 ？ \\n', '\\u3000 \\u3000 作者 试图 讲述 一部 在 光年 尺度 上 重新 演绎 的 中国 现代史 ， 讲述 一个 文明 二百次 毁灭 与 重生 的 传奇 。 \\n']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[['《', '三体', '》', '\\n'], ['作者', '：', '刘慈欣', '\\n'], ['正文', '\\n'], ['前言', '\\n'], ['\\u3000', '\\u3000', '《', '三体', '》', '终于', '能', '与', '科幻', '朋友', '们', '见面', '了', '，', '用', '连载', '的', '方式', '事先', '谁', '都', '没有', '想到', '，', '也', '是', '无奈', '之', '举', '。', '之前', '就', '题材', '问题', '与', '编辑', '们', '仔细', '商讨', '过', '，', '感觉', '没有', '什么', '问题', '，', '但', '没想到', '今年', '是', '文革', '三十周年', '这', '事儿', '，', '单行本', '一时', '出', '不了', '，', '也', '只能', '这样', '了', '。', '\\n'], ['\\u3000', '\\u3000', '其实', '这', '本书', '不是', '文革', '题材', '的', '，', '文革', '内容', '在', '其中', '只', '占', '不到', '十分之一', '，', '但', '却是', '一个', '漂荡', '在', '故事', '中', '挥之不去', '的', '精神', '幽灵', '。', '\\n'], ['\\u3000', '\\u3000', '本书', '虽', '不是', '《', '球状', '闪电', '》', '的', '续集', '，', '但', '可以', '看做', '那个', '故事', '所', '发生', '的', '世界', '在', '其后', '的', '延续', '，', '那个', '物理学家', '在', '故事', '中', '出现', '但', '已', '不', '重要', '，', '其他', '的', '人', '则', '永远', '消失', '了', '，', '林云', '真的', '死', '了', '，', '虽然', '我', '有时', '在', '想', '，', '如果', '她', '活', '下来', '，', '最后', '是不是', '这个', '主人公', '的', '样子', '？', '\\n'], ['\\u3000', '\\u3000', '这是', '一个', '暂', '名为', '《', '地球', '往事', '》', '的', '系列', '的', '第一部', '，', '可以', '看做', '一个', '更长', '的', '故事', '的', '开始', '。', '\\n'], ['\\u3000', '\\u3000', '这是', '一个', '关于', '背叛', '的', '故事', '，', '也', '是', '一个', '生存', '与', '死亡', '的', '故事', '，', '有时候', '，', '比起', '生存', '还是', '死亡', '来', '，', '忠诚', '与', '背叛', '可能', '更是', '一个', '问题', '。', '\\n'], ['\\u3000', '\\u3000', '疯狂', '与', '偏执', '，', '最终', '将', '在', '人类文明', '的', '内部', '异化', '出', '怎样', '的', '力量', '？', '冷酷', '的', '星空', '将', '如何', '拷问', '心中', '道德', '？', '\\n'], ['\\u3000', '\\u3000', '作者', '试图', '讲述', '一部', '在', '光年', '尺度', '上', '重新', '演绎', '的', '中国', '现代史', '，', '讲述', '一个', '文明', '二百次', '毁灭', '与', '重生', '的', '传奇', '。', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "seg_example = [' '.join(jieba.cut(sent)) for sent in example_list]\n",
    "print(seg_example)\n",
    "print(\"\\n\\n\\n\")\n",
    "seg_example = [jieba.lcut(sent) for sent in example_list]\n",
    "print(seg_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    从这个简单的例子我们可以看到，jieba工具包非常的强大，能够非常精确地将每一个句子切分成单个词语。中文分词实际上是一个自然语言处理非常困难的问题，涉及到命名实体识别（Named-Entity Recognition）等多方面的技术作底层支持。有的读者可能会觉得这个输出非常的乱。不要着急，接下来我们会通过一些方法将其“清洗干净”，让它看起来更符合我们的阅读习惯。\n",
    "\n",
    "    对于一般的分词任务，通常只需要简单地调用 **jieba.cut()** 或 **jieba.lcut()** 函数就可以满足要求，jieba会使用内建分词器进行切分，默认为 **jieba.dt**。jieba.cut()返回的是Tokenizer对象，一般需要插入一些字符使其显示结果。jieba.lcut()则会直接返回列表。对于一些特殊场景下的分词需求，可能会采用到自定义词典、自定义分词器等功能，我们将在后面进一步讨论。\n",
    "    \n",
    "    值得一提的是，jieba以分词为代表功能，但其实它也包含了许多其他功能，如关键词抽取等，具体可以参考jieba的github官方文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 文本清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    仍然延续上面的例子，我们希望只看到汉字，而不显示其他字符，那么可以这样做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['《 三体 》 \\n',\n",
       " '作者 ： 刘慈欣 \\n',\n",
       " '正文 \\n',\n",
       " '前言 \\n',\n",
       " '\\u3000 \\u3000 《 三体 》 终于 能 与 科幻 朋友 们 见面 了 ， 用 连载 的 方式 事先 谁 都 没有 想到 ， 也 是 无奈 之 举 。 之前 就 题材 问题 与 编辑 们 仔细 商讨 过 ， 感觉 没有 什么 问题 ， 但 没想到 今年 是 文革 三十周年 这 事儿 ， 单行本 一时 出 不了 ， 也 只能 这样 了 。 \\n',\n",
       " '\\u3000 \\u3000 其实 这 本书 不是 文革 题材 的 ， 文革 内容 在 其中 只 占 不到 十分之一 ， 但 却是 一个 漂荡 在 故事 中 挥之不去 的 精神 幽灵 。 \\n',\n",
       " '\\u3000 \\u3000 本书 虽 不是 《 球状 闪电 》 的 续集 ， 但 可以 看做 那个 故事 所 发生 的 世界 在 其后 的 延续 ， 那个 物理学家 在 故事 中 出现 但 已 不 重要 ， 其他 的 人 则 永远 消失 了 ， 林云 真的 死 了 ， 虽然 我 有时 在 想 ， 如果 她 活 下来 ， 最后 是不是 这个 主人公 的 样子 ？ \\n',\n",
       " '\\u3000 \\u3000 这是 一个 暂 名为 《 地球 往事 》 的 系列 的 第一部 ， 可以 看做 一个 更长 的 故事 的 开始 。 \\n',\n",
       " '\\u3000 \\u3000 这是 一个 关于 背叛 的 故事 ， 也 是 一个 生存 与 死亡 的 故事 ， 有时候 ， 比起 生存 还是 死亡 来 ， 忠诚 与 背叛 可能 更是 一个 问题 。 \\n',\n",
       " '\\u3000 \\u3000 疯狂 与 偏执 ， 最终 将 在 人类文明 的 内部 异化 出 怎样 的 力量 ？ 冷酷 的 星空 将 如何 拷问 心中 道德 ？ \\n',\n",
       " '\\u3000 \\u3000 作者 试图 讲述 一部 在 光年 尺度 上 重新 演绎 的 中国 现代史 ， 讲述 一个 文明 二百次 毁灭 与 重生 的 传奇 。 \\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_example = [' '.join(jieba.cut(sent)) for sent in example_list]\n",
    "seg_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def pre_repl(string):\n",
    "    '''把非中文字符去除'''\n",
    "\n",
    "    pattern = '[^\\u4e00-\\u9fa5]'\n",
    "    return re.sub(pattern, '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['三体'], ['作者', '刘慈欣'], ['正文'], ['前言'], ['三体', '终于', '能', '与', '科幻', '朋友', '们', '见面', '了', '用', '连载', '的', '方式', '事先', '谁', '都', '没有', '想到', '也', '是', '无奈', '之', '举', '之前', '就', '题材', '问题', '与', '编辑', '们', '仔细', '商讨', '过', '感觉', '没有', '什么', '问题', '但', '没想到', '今年', '是', '文革', '三十周年', '这', '事儿', '单行本', '一时', '出', '不了', '也', '只能', '这样', '了'], ['其实', '这', '本书', '不是', '文革', '题材', '的', '文革', '内容', '在', '其中', '只', '占', '不到', '十分之一', '但', '却是', '一个', '漂荡', '在', '故事', '中', '挥之不去', '的', '精神', '幽灵'], ['本书', '虽', '不是', '球状', '闪电', '的', '续集', '但', '可以', '看做', '那个', '故事', '所', '发生', '的', '世界', '在', '其后', '的', '延续', '那个', '物理学家', '在', '故事', '中', '出现', '但', '已', '不', '重要', '其他', '的', '人', '则', '永远', '消失', '了', '林云', '真的', '死', '了', '虽然', '我', '有时', '在', '想', '如果', '她', '活', '下来', '最后', '是不是', '这个', '主人公', '的', '样子'], ['这是', '一个', '暂', '名为', '地球', '往事', '的', '系列', '的', '第一部', '可以', '看做', '一个', '更长', '的', '故事', '的', '开始'], ['这是', '一个', '关于', '背叛', '的', '故事', '也', '是', '一个', '生存', '与', '死亡', '的', '故事', '有时候', '比起', '生存', '还是', '死亡', '来', '忠诚', '与', '背叛', '可能', '更是', '一个', '问题'], ['疯狂', '与', '偏执', '最终', '将', '在', '人类文明', '的', '内部', '异化', '出', '怎样', '的', '力量', '冷酷', '的', '星空', '将', '如何', '拷问', '心中', '道德'], ['作者', '试图', '讲述', '一部', '在', '光年', '尺度', '上', '重新', '演绎', '的', '中国', '现代史', '讲述', '一个', '文明', '二百次', '毁灭', '与', '重生', '的', '传奇']]\n"
     ]
    }
   ],
   "source": [
    "clean_example = [pre_repl(token) for token in seg_example]\n",
    "clean_example = [jieba.lcut(sent) for sent in clean_example]\n",
    "print(clean_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    怎么样？是不是看起来清爽多了？这样jieba强大的分词能力就更加明显了，而清洗过后的文本在之后的分析中也更便于使用。\n",
    "    \n",
    "    我们定义的清洗方法中，使用到了字符串处理库re，这可以看作是python对正则表达式的封装，对各种匹配需求都提供了灵活的支持。匹配规则有很多，大家不必记住，只要根据具体的匹配需求现场设计与测试即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 初窥门径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    有些好奇心强的同学可能会问，拿到这样分词的结果之后，还能做些什么事情呢？尽管越过了我们当前讨论的范畴，但我们还是稍微向前延申一下，看看还有什么名堂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    首先来试试统计一下词频："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'的': 19, '在': 7, '一个': 7, '与': 6, '故事': 6, '了': 4, '但': 4, '也': 3, '是': 3, '问题': 3, '文革': 3, '三体': 2, '作者': 2, '们': 2, '没有': 2, '题材': 2, '这': 2, '出': 2, '本书': 2, '不是': 2, '中': 2, '可以': 2, '看做': 2, '那个': 2, '这是': 2, '背叛': 2, '生存': 2, '死亡': 2, '将': 2, '讲述': 2, '刘慈欣': 1, '正文': 1, '前言': 1, '终于': 1, '能': 1, '科幻': 1, '朋友': 1, '见面': 1, '用': 1, '连载': 1, '方式': 1, '事先': 1, '谁': 1, '都': 1, '想到': 1, '无奈': 1, '之': 1, '举': 1, '之前': 1, '就': 1, '编辑': 1, '仔细': 1, '商讨': 1, '过': 1, '感觉': 1, '什么': 1, '没想到': 1, '今年': 1, '三十周年': 1, '事儿': 1, '单行本': 1, '一时': 1, '不了': 1, '只能': 1, '这样': 1, '其实': 1, '内容': 1, '其中': 1, '只': 1, '占': 1, '不到': 1, '十分之一': 1, '却是': 1, '漂荡': 1, '挥之不去': 1, '精神': 1, '幽灵': 1, '虽': 1, '球状': 1, '闪电': 1, '续集': 1, '所': 1, '发生': 1, '世界': 1, '其后': 1, '延续': 1, '物理学家': 1, '出现': 1, '已': 1, '不': 1, '重要': 1, '其他': 1, '人': 1, '则': 1, '永远': 1, '消失': 1, '林云': 1, '真的': 1, '死': 1, '虽然': 1, '我': 1, '有时': 1, '想': 1, '如果': 1, '她': 1, '活': 1, '下来': 1, '最后': 1, '是不是': 1, '这个': 1, '主人公': 1, '样子': 1, '暂': 1, '名为': 1, '地球': 1, '往事': 1, '系列': 1, '第一部': 1, '更长': 1, '开始': 1, '关于': 1, '有时候': 1, '比起': 1, '还是': 1, '来': 1, '忠诚': 1, '可能': 1, '更是': 1, '疯狂': 1, '偏执': 1, '最终': 1, '人类文明': 1, '内部': 1, '异化': 1, '怎样': 1, '力量': 1, '冷酷': 1, '星空': 1, '如何': 1, '拷问': 1, '心中': 1, '道德': 1, '试图': 1, '一部': 1, '光年': 1, '尺度': 1, '上': 1, '重新': 1, '演绎': 1, '中国': 1, '现代史': 1, '文明': 1, '二百次': 1, '毁灭': 1, '重生': 1, '传奇': 1})\n",
      "The top 10 words:\n",
      "[('的', 19), ('在', 7), ('一个', 7), ('与', 6), ('故事', 6), ('了', 4), ('但', 4), ('也', 3), ('是', 3), ('问题', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = [word for lis in clean_example for word in lis]\n",
    "cntr = Counter(bag_of_words)\n",
    "\n",
    "print(cntr)\n",
    "print(\"The top 10 words:\")\n",
    "print(cntr.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    用python3内建模块中的collections中的计数器Counter类可以很方便地进行词频统计，不妨再把频率分布画出来试试看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1fe6a993cf8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "top10 = cntr.most_common(10)\n",
    "sns.barplot(x=[each[0] for each in top10], y=[each[1] for each in top10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    这一步虽然简单，但也是文本挖掘任务中的一个基础工作。接下来我们再使用gensim库，稍微做一点事情："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'三体': 0, '作者': 1, '刘慈欣': 2, '正文': 3, '前言': 4, '一时': 5, '三十周年': 6, '不了': 7, '与': 8, '举': 9, '之': 10, '之前': 11, '也': 12, '了': 13, '事儿': 14, '事先': 15, '什么': 16, '今年': 17, '仔细': 18, '们': 19, '但': 20, '出': 21, '单行本': 22, '只能': 23, '商讨': 24, '就': 25, '想到': 26, '感觉': 27, '文革': 28, '方式': 29, '无奈': 30, '是': 31, '朋友': 32, '没想到': 33, '没有': 34, '用': 35, '的': 36, '科幻': 37, '终于': 38, '编辑': 39, '能': 40, '见面': 41, '谁': 42, '过': 43, '这': 44, '这样': 45, '连载': 46, '都': 47, '问题': 48, '题材': 49, '一个': 50, '不到': 51, '不是': 52, '中': 53, '其中': 54, '其实': 55, '内容': 56, '十分之一': 57, '占': 58, '却是': 59, '只': 60, '在': 61, '幽灵': 62, '挥之不去': 63, '故事': 64, '本书': 65, '漂荡': 66, '精神': 67, '下来': 68, '不': 69, '世界': 70, '主人公': 71, '人': 72, '其他': 73, '其后': 74, '出现': 75, '则': 76, '发生': 77, '可以': 78, '她': 79, '如果': 80, '已': 81, '延续': 82, '想': 83, '我': 84, '所': 85, '是不是': 86, '最后': 87, '有时': 88, '林云': 89, '样子': 90, '死': 91, '永远': 92, '活': 93, '消失': 94, '物理学家': 95, '球状': 96, '看做': 97, '真的': 98, '续集': 99, '虽': 100, '虽然': 101, '这个': 102, '那个': 103, '重要': 104, '闪电': 105, '名为': 106, '地球': 107, '开始': 108, '往事': 109, '暂': 110, '更长': 111, '第一部': 112, '系列': 113, '这是': 114, '关于': 115, '可能': 116, '忠诚': 117, '更是': 118, '有时候': 119, '来': 120, '死亡': 121, '比起': 122, '生存': 123, '背叛': 124, '还是': 125, '人类文明': 126, '偏执': 127, '内部': 128, '冷酷': 129, '力量': 130, '如何': 131, '将': 132, '异化': 133, '心中': 134, '怎样': 135, '拷问': 136, '星空': 137, '最终': 138, '疯狂': 139, '道德': 140, '一部': 141, '上': 142, '中国': 143, '二百次': 144, '传奇': 145, '光年': 146, '尺度': 147, '文明': 148, '毁灭': 149, '演绎': 150, '现代史': 151, '讲述': 152, '试图': 153, '重新': 154, '重生': 155}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(clean_example)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    像这样，我们创建了一个“词典”，为每个词语分配了一个唯一id。这样形式的表示将有助于计算机理解和处理这些词语，在这个基础上，我们创建对应的语料集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1)], [(1, 1), (2, 1)], [(3, 1)], [(4, 1)], [(0, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 2), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 2), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 2), (49, 1)], [(20, 1), (28, 2), (36, 2), (44, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 2), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1)], [(13, 2), (20, 2), (36, 5), (52, 1), (53, 1), (61, 3), (64, 2), (65, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 2), (104, 1), (105, 1)], [(36, 4), (50, 2), (64, 1), (78, 1), (97, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1)], [(8, 2), (12, 1), (31, 1), (36, 2), (48, 1), (50, 3), (64, 2), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 2), (122, 1), (123, 2), (124, 2), (125, 1)], [(8, 1), (21, 1), (36, 3), (61, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 2), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1)], [(1, 1), (8, 1), (36, 2), (50, 1), (61, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 2), (153, 1), (154, 1), (155, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(each) for each in clean_example]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    我们调用了词典的一项方法doc2bow，将clean_example这个二重列表转化成为一个词袋模型（Bag-of-Words）的表示。所谓“词袋”，顾名思义，就是把所有词语一股脑装进一个包里，忽略他们的先后顺序和语义，仅仅强调他们作为显式存在的一个个词语，突出他们出现的频率不通。\n",
    "    \n",
    "    观察输出结果，原来的一个个列表中的词语变成了二元组。这个二元组的含义是（词语id，出现频数），如（0，1）表示“三体”一词出现了一次，以此类推。现在你明白为什么要先有词典，才能进行建模了吧！将doc2bow设计成字典类的一个方法，也是有合理的考虑的。\n",
    "    \n",
    "    事实上，还能做更多的事情，让我们不加说明地展示一二："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-25 17:28:34,482 : INFO : collecting document frequencies\n",
      "2019-07-25 17:28:34,484 : INFO : PROGRESS: processing document #0\n",
      "2019-07-25 17:28:34,486 : INFO : calculating IDF weights for 11 documents and 155 features (192 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x000001FE671F95F8>\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "print(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    观察输出日志信息，模型对11个文档，155个特征进行了计算。也就是说，corpus这个二维列表中的每一个一维列表都会被认为是一个document切分成的词语，词语的总共数量就是特征的数量。我们打印出结果看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0个： [(0, 1.0)]\n",
      "第1个： [(1, 0.5794285475904607), (2, 0.8150230415376054)]\n",
      "第2个： [(3, 1.0)]\n",
      "第3个： [(4, 1.0)]\n",
      "第4个： [(0, 0.09891763068075399), (5, 0.1391373423942927), (6, 0.1391373423942927), (7, 0.1391373423942927), (8, 0.11739583793443054), (9, 0.1391373423942927), (10, 0.1391373423942927), (11, 0.1391373423942927), (12, 0.19783526136150797), (13, 0.19783526136150797), (14, 0.1391373423942927), (15, 0.1391373423942927), (16, 0.1391373423942927), (17, 0.1391373423942927), (18, 0.1391373423942927), (19, 0.2782746847885854), (20, 0.07539060753851841), (21, 0.09891763068075399), (22, 0.1391373423942927), (23, 0.1391373423942927), (24, 0.1391373423942927), (25, 0.1391373423942927), (26, 0.1391373423942927), (27, 0.1391373423942927), (28, 0.09891763068075399), (29, 0.1391373423942927), (30, 0.1391373423942927), (31, 0.19783526136150797), (32, 0.1391373423942927), (33, 0.1391373423942927), (34, 0.2782746847885854), (35, 0.1391373423942927), (36, 0.026226336751551914), (37, 0.1391373423942927), (38, 0.1391373423942927), (39, 0.1391373423942927), (40, 0.1391373423942927), (41, 0.1391373423942927), (42, 0.1391373423942927), (43, 0.1391373423942927), (44, 0.09891763068075399), (45, 0.1391373423942927), (46, 0.1391373423942927), (47, 0.1391373423942927), (48, 0.19783526136150797), (49, 0.09891763068075399)]\n",
      "第5个： [(20, 0.12752808647830968), (28, 0.3346511341811267), (36, 0.08872708817348025), (44, 0.16732556709056334), (49, 0.16732556709056334), (50, 0.09929132461657507), (51, 0.23535980956455166), (52, 0.16732556709056334), (53, 0.16732556709056334), (54, 0.23535980956455166), (55, 0.23535980956455166), (56, 0.23535980956455166), (57, 0.23535980956455166), (58, 0.23535980956455166), (59, 0.23535980956455166), (60, 0.23535980956455166), (61, 0.19858264923315014), (62, 0.23535980956455166), (63, 0.23535980956455166), (64, 0.09929132461657507), (65, 0.16732556709056334), (66, 0.23535980956455166), (67, 0.23535980956455166)]\n",
      "第6个： [(13, 0.20538953098774115), (20, 0.15653874255431297), (36, 0.1361389008171993), (52, 0.10269476549387058), (53, 0.10269476549387058), (61, 0.18281783485406192), (64, 0.12187855656937462), (65, 0.10269476549387058), (68, 0.14445025270305387), (69, 0.14445025270305387), (70, 0.14445025270305387), (71, 0.14445025270305387), (72, 0.14445025270305387), (73, 0.14445025270305387), (74, 0.14445025270305387), (75, 0.14445025270305387), (76, 0.14445025270305387), (77, 0.14445025270305387), (78, 0.10269476549387058), (79, 0.14445025270305387), (80, 0.14445025270305387), (81, 0.14445025270305387), (82, 0.14445025270305387), (83, 0.14445025270305387), (84, 0.14445025270305387), (85, 0.14445025270305387), (86, 0.14445025270305387), (87, 0.14445025270305387), (88, 0.14445025270305387), (89, 0.14445025270305387), (90, 0.14445025270305387), (91, 0.14445025270305387), (92, 0.14445025270305387), (93, 0.14445025270305387), (94, 0.14445025270305387), (95, 0.14445025270305387), (96, 0.14445025270305387), (97, 0.10269476549387058), (98, 0.14445025270305387), (99, 0.14445025270305387), (100, 0.14445025270305387), (101, 0.14445025270305387), (102, 0.14445025270305387), (103, 0.28890050540610773), (104, 0.14445025270305387), (105, 0.14445025270305387)]\n",
      "第7个： [(36, 0.2275930423321131), (50, 0.25469126860658214), (64, 0.12734563430329107), (78, 0.2146026408509472), (97, 0.2146026408509472), (106, 0.3018596473986034), (107, 0.3018596473986034), (108, 0.3018596473986034), (109, 0.3018596473986034), (110, 0.3018596473986034), (111, 0.3018596473986034), (112, 0.3018596473986034), (113, 0.3018596473986034), (114, 0.2146026408509472)]\n",
      "第8个： [(8, 0.16811252721666298), (12, 0.14165146884776342), (31, 0.14165146884776342), (36, 0.07511298234271642), (48, 0.14165146884776342), (50, 0.2521687908249945), (64, 0.16811252721666298), (114, 0.14165146884776342), (115, 0.19924667408719537), (116, 0.19924667408719537), (117, 0.19924667408719537), (118, 0.19924667408719537), (119, 0.19924667408719537), (120, 0.19924667408719537), (121, 0.39849334817439075), (122, 0.19924667408719537), (123, 0.39849334817439075), (124, 0.39849334817439075), (125, 0.19924667408719537)]\n",
      "第9个： [(8, 0.09632560904120371), (21, 0.16232774837483346), (36, 0.12911537095159778), (61, 0.09632560904120371), (126, 0.22832988770846327), (127, 0.22832988770846327), (128, 0.22832988770846327), (129, 0.22832988770846327), (130, 0.22832988770846327), (131, 0.22832988770846327), (132, 0.45665977541692654), (133, 0.22832988770846327), (134, 0.22832988770846327), (135, 0.22832988770846327), (136, 0.22832988770846327), (137, 0.22832988770846327), (138, 0.22832988770846327), (139, 0.22832988770846327), (140, 0.22832988770846327)]\n",
      "第10个： [(1, 0.16232636218996127), (8, 0.09632478647634098), (36, 0.08607617892078404), (50, 0.09632478647634098), (61, 0.09632478647634098), (141, 0.22832793790358158), (142, 0.22832793790358158), (143, 0.22832793790358158), (144, 0.22832793790358158), (145, 0.22832793790358158), (146, 0.22832793790358158), (147, 0.22832793790358158), (148, 0.22832793790358158), (149, 0.22832793790358158), (150, 0.22832793790358158), (151, 0.22832793790358158), (152, 0.45665587580716316), (153, 0.22832793790358158), (154, 0.22832793790358158), (155, 0.22832793790358158)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for i, doc in enumerate(corpus_tfidf):\n",
    "    print(\"第%i个：\" % i, doc)\n",
    "    # print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    对比词袋模型语料，我们看到，这里二元组变为了（词语id，词语的tf-idf权重值）。至于tf-idf是什么，有什么用，我们会在后面的内容进行讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.本章总结\n",
    "- 应用jieba工具包进行中文分词\n",
    "- 应用re和正则表达式对文本进行清洗\n",
    "- 应用gensim库构建词典和tf-idf模型\n",
    "\n",
    "参考链接：\n",
    "\n",
    "[https://github.com/fxsjy/jieba]\n",
    "\n",
    "[https://radimrehurek.com/gensim/]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
